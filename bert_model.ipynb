{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c02fb1",
   "metadata": {},
   "source": [
    "# Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc250e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.1->torch) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c9cbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim import AdamW # Corrected AdamW import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a48164",
   "metadata": {},
   "source": [
    "# Important Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d06c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model and Tokenizer names\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3 # This will be adjusted for gradual unfreezing later\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba731cdf",
   "metadata": {},
   "source": [
    "# Importing Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ffd396a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from kagglehub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kagglehub) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (2023.7.22)\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m489.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d5f2a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/jovyan/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "data = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02d5688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV file: /home/jovyan/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_file_path = os.path.join(dirname, filename)\n",
    "            print(f\"Found CSV file: {csv_file_path}\")\n",
    "            # Load the CSV file into a pandas DataFrame\n",
    "            data = pd.read_csv(csv_file_path)\n",
    "            display(data.head())\n",
    "            break # Assuming there's only one CSV file of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e3069",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf5498f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b7f395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cb8cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "imdb_data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe25838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "427c0a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m825.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf355315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import contractions\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9458b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = imdb_data.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8641616f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ba242d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_contractions(text):\n",
    "  expanded_text = contractions.fix(text)\n",
    "  return expanded_text\n",
    "\n",
    "def remove_punc(text):\n",
    "    unwanted = set(string.punctuation + string.digits)\n",
    "    return ''.join(char for char in str(text) if char not in unwanted)\n",
    "\n",
    "def remove_punc(text):\n",
    "    # Ensure input is a string; handle potential NaN/None\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Keep digits for now if not explicitly asked to remove, but previous context suggests removing\n",
    "    unwanted = set(string.punctuation + string.digits) # Includes digits as per previous conversation\n",
    "    return ''.join(char for char in str(text) if char not in unwanted)\n",
    "\n",
    "def remove_stopwords(words): # Expects a list of words\n",
    "    if not isinstance(words, list): # Handle cases where input might not be a list (e.g., NaN after tokenization failed)\n",
    "        return words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "def lemmatize_words(words): # Expects a list of words\n",
    "    if not isinstance(words, list): # Handle cases where input might not be a list\n",
    "        return words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87acc969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73c616e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing to 'review' column...\n",
      "- Removing HTML tags...\n",
      "- Removing URLs...\n",
      "- Removing contractions...\n",
      "- Removing punctuation and digits...\n",
      "- Removing stopwords...\n",
      "- Lemmatizing words...\n",
      "\n",
      "All text preprocessing steps completed for 'review'! ✅\n",
      "\n",
      "First 5 rows of the processed 'review' column:\n",
      "0    one of the other reviewers has mentioned that ...\n",
      "1    a wonderful little production the filming tech...\n",
      "2    i thought this was a wonderful way to spend ti...\n",
      "3    basically there is a family where a little boy...\n",
      "4    petter matteis love in the time of money is a ...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Apply the functions sequentially using .loc for 'review' ---\n",
    "\n",
    "print(\"Applying text preprocessing to 'review' column...\")\n",
    "\n",
    "# Step 1: Convert review column to string type\n",
    "imdb_dataset.loc[:, 'review'] = imdb_dataset['review'].astype(str)\n",
    "\n",
    "# Step 2: Convert to lowercase\n",
    "imdb_dataset.loc[:, 'review'] = imdb_dataset['review'].str.lower()\n",
    "\n",
    "# Step 3: Remove HTML tags\n",
    "print(\"- Removing HTML tags...\")\n",
    "imdb_dataset.loc[:, 'review'] = imdb_dataset['review'].apply(remove_html_tags)\n",
    "\n",
    "# Step 4: Remove URLs\n",
    "print(\"- Removing URLs...\")\n",
    "imdb_dataset.loc[:, 'review'] = imdb_dataset['review'].apply(remove_url)\n",
    "\n",
    "# Step 5: Remove contractions\n",
    "print(\"- Removing contractions...\")\n",
    "imdb_dataset.loc[:, 'review'] = imdb_dataset['review'].apply(remove_contractions)\n",
    "\n",
    "# Step 6: Remove punctuation and digits\n",
    "print(\"- Removing punctuation and digits...\")\n",
    "imdb_dataset.loc[:, 'review'] = imdb_dataset['review'].apply(remove_punc)\n",
    "\n",
    "# Step 7: Remove stopwords\n",
    "print(\"- Removing stopwords...\")\n",
    "imdb_dataset.loc[:, 'review'] = imdb_dataset['review'].apply(remove_stopwords)\n",
    "\n",
    "# Step 8: Lemmatize words\n",
    "print(\"- Lemmatizing words...\")\n",
    "imdb_dataset.loc[:, 'review'] = imdb_dataset['review'].apply(lemmatize_words)\n",
    "\n",
    "print(\"\\nAll text preprocessing steps completed for 'review'! ✅\")\n",
    "print(\"\\nFirst 5 rows of the processed 'review' column:\")\n",
    "print(imdb_dataset['review'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7986d7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    one of the other reviewers has mentioned that ...\n",
      "1    a wonderful little production the filming tech...\n",
      "2    i thought this was a wonderful way to spend ti...\n",
      "3    basically there is a family where a little boy...\n",
      "4    petter matteis love in the time of money is a ...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(imdb_dataset['review'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772ed33",
   "metadata": {},
   "source": [
    "# Data-Splitting for Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4b8c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split complete with 'sentiment' as the target.\n",
      "Total Features (X) used: 1 columns\n",
      "Target (y) used: sentiment\n",
      "\n",
      "Training set samples (features): 800 rows\n",
      "Training set samples (target): 800 rows\n",
      "Test set samples (features): 200 rows\n",
      "Test set samples (target): 200 rows\n",
      "\n",
      "First 3 rows of X_train (features for training):\n",
      "                                                review\n",
      "298  going into see seven pounds i was not clearly ...\n",
      "929  i saw this movie in a theater while on vacatio...\n",
      "1    a wonderful little production the filming tech...\n",
      "\n",
      "First 3 values of y_train (target for training):\n",
      "298    positive\n",
      "929    positive\n",
      "1      positive\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Assumed: imdb_data is already loaded from your previous steps ---\n",
    "# Make sure 'imdb_data' is the name of your DataFrame with 'review' and 'sentiment' columns\n",
    "# If not, you'll need to ensure the variable name matches.\n",
    "# For example, if you loaded it as 'df', then use 'df' below.\n",
    "\n",
    "# 1. Define your target variable (y)\n",
    "# This will be the column you want to predict.\n",
    "target_column = 'sentiment' # Changed to 'sentiment'\n",
    "y = imdb_dataset[target_column]\n",
    "\n",
    "# 2. Define your features (X)\n",
    "# This selects all columns EXCEPT the target_column.\n",
    "# In this case, 'review' is your feature.\n",
    "X = imdb_dataset.drop(columns=[target_column]) # 'review' is implicitly selected as it's the only other column\n",
    "\n",
    "# 3. Perform the train-test split\n",
    "# We'll use 20% of the data for testing (test_size=0.20) and 80% for training.\n",
    "# random_state ensures your split is reproducible.\n",
    "# Since 'sentiment' is a categorical value (classification task), it's good practice to use 'stratify'\n",
    "# to ensure that the proportions of each sentiment class are maintained in both train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y # Added stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Data split complete with '{target_column}' as the target.\")\n",
    "print(f\"Total Features (X) used: {len(X.columns)} columns\")\n",
    "print(f\"Target (y) used: {target_column}\")\n",
    "\n",
    "print(f\"\\nTraining set samples (features): {len(X_train)} rows\")\n",
    "print(f\"Training set samples (target): {len(y_train)} rows\")\n",
    "print(f\"Test set samples (features): {len(X_test)} rows\")\n",
    "print(f\"Test set samples (target): {len(y_test)} rows\")\n",
    "\n",
    "print(\"\\nFirst 3 rows of X_train (features for training):\")\n",
    "print(X_train.head(3))\n",
    "print(\"\\nFirst 3 values of y_train (target for training):\")\n",
    "print(y_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33baee24",
   "metadata": {},
   "source": [
    "# Bert-Tokenizer & important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe8bdd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"Tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8d518a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auxiliary_sentence(label):\n",
    "    if label == 1:\n",
    "        return \"This review expresses positive sentiment.\"\n",
    "    else:\n",
    "        return \"This review expresses negative sentiment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc91c8",
   "metadata": {},
   "source": [
    "# DataLoaders And Datasets for making data batches etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59feb2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and DataLoaders created successfully using the provided train-test split.\n",
      "Number of samples in training dataset: 800\n",
      "Number of samples in test dataset: 200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer # Assuming you'll use a Hugging Face tokenizer\n",
    "\n",
    "# --- Assumed: imdb_data is already loaded and X_train, X_test, y_train, y_test are defined ---\n",
    "# If you need to re-run the data loading and splitting for testing,\n",
    "# make sure to use the correct 'imdb_data' DataFrame as established previously.\n",
    "\n",
    "# Example of how imdb_data might look and how to split it if you're testing this block in isolation:\n",
    "# data = {'review': [\"This movie was great!\", \"Terrible film, avoid at all costs.\", \"Decent acting, weak plot.\"],\n",
    "#         'sentiment': [\"positive\", \"negative\", \"negative\"]}\n",
    "# imdb_data = pd.DataFrame(data)\n",
    "#\n",
    "# target_column = 'sentiment'\n",
    "# X = imdb_data.drop(columns=[target_column])\n",
    "# y = imdb_data[target_column]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 1. Identify the text column for 'review' and convert sentiment labels ---\n",
    "text_column = 'review' # Directly set to 'review' as per your dataset\n",
    "\n",
    "train_texts = X_train[text_column].tolist()\n",
    "test_texts = X_test[text_column].tolist()\n",
    "\n",
    "# Convert sentiment labels to numerical format (e.g., positive: 1, negative: 0)\n",
    "# This is crucial for PyTorch models.\n",
    "# Assuming your sentiment is 'positive' and 'negative'. Adjust if different.\n",
    "sentiment_mapping = {'positive': 1, 'negative': 0}\n",
    "train_labels = [sentiment_mapping[label] for label in y_train.tolist()]\n",
    "test_labels = [sentiment_mapping[label] for label in y_test.tolist()]\n",
    "\n",
    "# --- 3. Define the IMDbBERT4TCDataset Class (Simplified for standard classification) ---\n",
    "class IMDbBERT4TCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # For standard sentiment classification, we just encode the text.\n",
    "        # No need for get_auxiliary_sentence unless specifically required by your model architecture.\n",
    "        encoded_input = self.tokenizer(\n",
    "            text, # Only passing the text here\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoded_input['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded_input['attention_mask'].squeeze(),\n",
    "            'token_type_ids': encoded_input['token_type_ids'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 4. Create Datasets and DataLoaders ---\n",
    "train_dataset = IMDbBERT4TCDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "test_dataset = IMDbBERT4TCDataset(test_texts, test_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Datasets and DataLoaders created successfully using the provided train-test split.\")\n",
    "print(f\"Number of samples in training dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in test dataset: {len(test_dataset)}\")\n",
    "\n",
    "# Optional: Verify a batch\n",
    "# for batch in train_dataloader:\n",
    "#     print(\"\\nSample batch structure:\")\n",
    "#     print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "#     print(f\"Attention Mask shape: {batch['attention_mask'].shape}\")\n",
    "#     print(f\"Token Type IDs shape: {batch['token_type_ids'].shape}\")\n",
    "#     print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "728902c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT4TC Model conceptual class defined and instantiated.\n"
     ]
    }
   ],
   "source": [
    "class BERT4TCModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "model = BERT4TCModel(MODEL_NAME, num_labels=2).to(device)\n",
    "print(\"BERT4TC Model conceptual class defined and instantiated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82885372",
   "metadata": {},
   "source": [
    "# Optimizer And Scheduler Setup, Learning Rate Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e906b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer with discriminative learning rates configured.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# --- Optimizer and Scheduler setup (REVISED for robust layer grouping) ---\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# Create separate lists for each parameter group\n",
    "embeddings_params = []\n",
    "encoder_layer_params_lower = [] # Layers 0-5\n",
    "encoder_layer_params_upper = [] # Layers 6-11\n",
    "pooler_classifier_params = []\n",
    "no_decay_params = [] # Parameters for which weight_decay is 0.0\n",
    "\n",
    "# Regex to find layer number, e.g., \"layer.X.\"\n",
    "layer_pattern = re.compile(r'bert\\.encoder\\.layer\\.(\\d+)\\.')\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if any(nd in n for nd in no_decay):\n",
    "        no_decay_params.append(p)\n",
    "    else:\n",
    "        # Check for embeddings\n",
    "        if 'bert.embeddings' in n:\n",
    "            embeddings_params.append(p)\n",
    "        # Check for encoder layers\n",
    "        elif 'bert.encoder.layer.' in n: # Using in for initial filter, then regex for precise parsing\n",
    "            match = layer_pattern.search(n)\n",
    "            if match:\n",
    "                layer_idx = int(match.group(1)) # Extract the number following 'layer.'\n",
    "                if 0 <= layer_idx < 6:\n",
    "                    encoder_layer_params_lower.append(p)\n",
    "                elif 6 <= layer_idx < 12: # BERT-base has 12 layers (0-11)\n",
    "                    encoder_layer_params_upper.append(p)\n",
    "                # else: print(f\"Warning: Layer index out of expected range for BERT-base: {layer_idx} in {n}\")\n",
    "            # else:\n",
    "            #     print(f\"Warning: 'bert.encoder.layer.' found but no layer number parsed for {n}\")\n",
    "        # Check for pooler and classifier\n",
    "        elif 'bert.pooler' in n or 'classifier' in n:\n",
    "            pooler_classifier_params.append(p)\n",
    "        # else:\n",
    "        #     # For debugging any parameters not categorized\n",
    "        #     # print(f\"Parameter not categorized: {n}\")\n",
    "        #     pass\n",
    "\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': embeddings_params,\n",
    "        'lr': LEARNING_RATE * 0.1, # Smallest LR for embeddings\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': encoder_layer_params_lower, # Layers 0-5\n",
    "        'lr': LEARNING_RATE * 0.3, # Mid-low LR\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': encoder_layer_params_upper, # Layers 6-11\n",
    "        'lr': LEARNING_RATE * 0.6, # Mid-high LR\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': pooler_classifier_params, # Pooler and classifier\n",
    "        'lr': LEARNING_RATE, # Highest LR\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': no_decay_params, # Bias and LayerNorm weights\n",
    "        'lr': 0.0, # Or a very small LR if preferred, but 0.0 is common with weight_decay=0\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters)\n",
    "print(\"Optimizer with discriminative learning rates configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05f46c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate scheduler configured.\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "print(\"Learning rate scheduler configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14609590",
   "metadata": {},
   "source": [
    "# Important Functions for Gradual Unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b078293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All BERT layers initially frozen (classifier/pooler trainable).\n"
     ]
    }
   ],
   "source": [
    "# --- Gradual Unfreezing helper functions ---\n",
    "def freeze_all_bert_layers(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bert\" in name:\n",
    "            param.requires_grad = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"classifier\" in name or \"pooler\" in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "def unfreeze_bert_layer_block(model, layer_index):\n",
    "    for name, param in model.named_parameters():\n",
    "        if f\"bert.encoder.layer.{layer_index}.\" in name:\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfrozen layer: {name}\")\n",
    "\n",
    "def unfreeze_bert_embeddings(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bert.embeddings\" in name:\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfrozen embeddings: {name}\")\n",
    "\n",
    "freeze_all_bert_layers(model)\n",
    "print(\"All BERT layers initially frozen (classifier/pooler trainable).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79b109eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==3.1.0 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.12.0)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.11/site-packages (from mlflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<3 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.24.4)\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.11/site-packages (from mlflow) (2.1.1)\n",
      "Requirement already satisfied: pyarrow<21,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow) (13.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.3.1)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.11.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow) (2.0.22)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (3.0.0)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading fastapi-0.115.13-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (3.1.40)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (6.8.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: packaging<26 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (23.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (4.24.3)\n",
      "Collecting pydantic<3,>=1.10.8 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (2.31.0)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==3.1.0->mlflow) (4.14.0)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.4)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow) (2.1.3)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (3.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3->mlflow) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3->mlflow) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (3.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.0)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.0->mlflow) (4.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.0->mlflow) (3.17.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.0->mlflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.0->mlflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.0->mlflow) (2023.7.22)\n",
      "Collecting h11>=0.8 (from uvicorn<1->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.0->mlflow) (3.0.5)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.11/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.0->mlflow) (4.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.0->mlflow) (1.3.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading mlflow-3.1.0-py3-none-any.whl (24.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.1.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m631.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m723.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m819.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m791.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m685.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading fastapi-0.115.13-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.1/216.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, typing-inspection, sqlparse, pydantic-core, pyasn1, itsdangerous, h11, gunicorn, graphql-core, cachetools, blinker, annotated-types, uvicorn, starlette, rsa, pydantic, pyasn1-modules, opentelemetry-api, graphql-relay, Flask, docker, opentelemetry-semantic-conventions, graphene, google-auth, fastapi, opentelemetry-sdk, databricks-sdk, mlflow-skinny, mlflow\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.6.3\n",
      "    Uninstalling blinker-1.6.3:\n",
      "      Successfully uninstalled blinker-1.6.3\n",
      "Successfully installed Flask-3.1.1 annotated-types-0.7.0 blinker-1.9.0 cachetools-5.5.2 databricks-sdk-0.57.0 docker-7.1.0 fastapi-0.115.13 google-auth-2.40.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 h11-0.16.0 itsdangerous-2.2.0 mlflow-3.1.0 mlflow-skinny-3.1.0 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.7 pydantic-core-2.33.2 rsa-4.9.1 sqlparse-0.5.3 starlette-0.46.2 typing-inspection-0.4.1 uvicorn-0.34.3 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6ec1b",
   "metadata": {},
   "source": [
    "# BertFinetuning on our standard way without auxiliary Sentences  -  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb082afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate scheduler configured.\n",
      "All BERT encoder layers frozen.\n",
      "\n",
      "--- Epoch 1/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e759b0a6ea6b484c93b5f26d6a2ce4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.7030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f7163cffb3459585e5084420dbd8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Epoch 1:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.6976\n",
      "Validation Accuracy: 0.5000\n",
      "New best model saved with accuracy: 0.5000 at mlflow_models/best_sentiment_model_final.pth\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Unfrozen BERT layer 11.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2652501fb0b4b49a4bd39a1070e5a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.7036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba994529ca6488997d3e0273d7c277e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Epoch 2:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.6976\n",
      "Validation Accuracy: 0.5000\n",
      "Validation accuracy did not improve. Best so far: 0.5000\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Unfrozen BERT layer 10.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db88bae2c784df8953e7657d31eee5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.7022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf75361c19549fc96cb89b40125a17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Epoch 3:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.6976\n",
      "Validation Accuracy: 0.5000\n",
      "Validation accuracy did not improve. Best so far: 0.5000\n",
      "\n",
      "Training complete.\n",
      "Final best validation accuracy: 0.5000\n",
      "Best model saved at: mlflow_models/best_sentiment_model_final.pth\n",
      "🏃 View run magnificent-lark-241 at: http://mlflow:5000/#/experiments/0/runs/7a15d36666c443b2a703a0d907029111\n",
      "🧪 View experiment at: http://mlflow:5000/#/experiments/0\n",
      "\n",
      "MLflow run completed.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os # Make sure os is imported\n",
    "\n",
    "# Define your total epochs\n",
    "NUM_FREEZE_EPOCHS = 1\n",
    "NUM_GRADUAL_UNFREEZE_EPOCHS = 1\n",
    "TOTAL_EPOCHS = NUM_FREEZE_EPOCHS + NUM_GRADUAL_UNFREEZE_EPOCHS + 1\n",
    "\n",
    "\n",
    "# Calculate total steps for the scheduler using the actual TOTAL_EPOCHS\n",
    "total_steps = len(train_dataloader) * TOTAL_EPOCHS\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "# Initialize Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "print(\"Learning rate scheduler configured.\")\n",
    "\n",
    "\n",
    "# --- Gradual Unfreezing Helper Functions (as defined previously) ---\n",
    "def freeze_all_bert_layers(model):\n",
    "    \"\"\"Freezes all parameters within the BERT encoder layers.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bert.\" in name: # Targets parameters belonging to the BERT base model\n",
    "            param.requires_grad = False\n",
    "    print(\"All BERT encoder layers frozen.\")\n",
    "\n",
    "def unfreeze_bert_layer_block(model, layer_num):\n",
    "    \"\"\"Unfreezes a specific BERT encoder layer block.\"\"\"\n",
    "    layer_prefix = f\"bert.encoder.layer.{layer_num}.\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if layer_prefix in name:\n",
    "            param.requires_grad = True\n",
    "    print(f\"Unfrozen BERT layer {layer_num}.\")\n",
    "\n",
    "def unfreeze_bert_embeddings(model):\n",
    "    \"\"\"Unfreezes BERT's embedding layer.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bert.embeddings.\" in name:\n",
    "            param.requires_grad = True\n",
    "    print(\"Unfrozen BERT embeddings.\")\n",
    "\n",
    "# --- Initial Freezing (Before the loop starts) ---\n",
    "freeze_all_bert_layers(model)\n",
    "\n",
    "# --- MLflow Setup ---\n",
    "# Set the MLflow tracking URI (if not set by MLFLOW_TRACKING_URI environment variable)\n",
    "# By default, it's 'mlruns' in your current directory or can be a remote server.\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\") # CORRECTED LINE: Removed duplicate \"http://\"\n",
    "\n",
    "# Ensure the directory for saving models exists\n",
    "model_save_dir = \"mlflow_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(model_save_dir, 'best_sentiment_model_final.pth')\n",
    "\n",
    "\n",
    "# --- Start MLflow Run ---\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"num_freeze_epochs\", NUM_FREEZE_EPOCHS)\n",
    "    mlflow.log_param(\"num_gradual_unfreeze_epochs\", NUM_GRADUAL_UNFREEZE_EPOCHS)\n",
    "    mlflow.log_param(\"total_epochs\", TOTAL_EPOCHS)\n",
    "    mlflow.log_param(\"warmup_steps_ratio\", 0.1) # Log the ratio used for warmup steps\n",
    "    mlflow.log_param(\"initial_learning_rate\", optimizer.param_groups[0]['lr']) # Log initial LR\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # --- Training Loop without Checkpointing ---\n",
    "    for epoch in range(TOTAL_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{TOTAL_EPOCHS} ---\")\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Gradual Unfreezing Logic\n",
    "        if epoch == 0:\n",
    "            pass\n",
    "        elif epoch == NUM_FREEZE_EPOCHS:\n",
    "            unfreeze_bert_layer_block(model, 11)\n",
    "        elif epoch == NUM_FREEZE_EPOCHS + 1:\n",
    "            unfreeze_bert_layer_block(model, 10)\n",
    "        elif epoch == NUM_FREEZE_EPOCHS + 2:\n",
    "            unfreeze_bert_layer_block(model, 9)\n",
    "            unfreeze_bert_embeddings(model)\n",
    "            print(\"All specified BERT layers and embeddings unfrozen for subsequent epochs.\")\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step() # This will now be defined\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        mlflow.log_metric(\"avg_train_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        # --- Evaluation Loop ---\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        for batch in tqdm(test_dataloader, desc=f\"Evaluating Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_eval_accuracy += (preds == labels).sum().item()\n",
    "\n",
    "        avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
    "        avg_eval_accuracy = total_eval_accuracy / len(test_dataset)\n",
    "        print(f\"Average Validation Loss: {avg_eval_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {avg_eval_accuracy:.4f}\")\n",
    "\n",
    "        mlflow.log_metric(\"avg_eval_loss\", avg_eval_loss, step=epoch)\n",
    "        mlflow.log_metric(\"avg_eval_accuracy\", avg_eval_accuracy, step=epoch)\n",
    "\n",
    "        # --- Save the best model ---\n",
    "        if avg_eval_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_eval_accuracy\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"New best model saved with accuracy: {best_accuracy:.4f} at {model_save_path}\")\n",
    "            # Log the best model as an artifact\n",
    "            mlflow.log_artifact(model_save_path, \"best_model\")\n",
    "            mlflow.set_tag(\"best_accuracy\", f\"{best_accuracy:.4f}\") # Set a tag for easy filtering\n",
    "        else:\n",
    "            print(f\"Validation accuracy did not improve. Best so far: {best_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    print(f\"Final best validation accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Best model saved at: {model_save_path}\")\n",
    "\n",
    "    # Log the final best accuracy as a metric at the end of the run\n",
    "    mlflow.log_metric(\"final_best_accuracy\", best_accuracy)\n",
    "\n",
    "print(\"\\nMLflow run completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809dde4",
   "metadata": {},
   "source": [
    "# BertFinetuning on our standard way without auxiliary Sentences  -  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35c23886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets and DataLoaders created successfully with auxiliary sentence input. ✅\n",
      "Number of samples in training dataset: 800\n",
      "Number of samples in test dataset: 200\n",
      "\n",
      "Verifying token_type_ids for a sample batch:\n",
      "Input IDs shape: torch.Size([16, 256])\n",
      "Attention Mask shape: torch.Size([16, 256])\n",
      "Token Type IDs shape (should show 0s and 1s): torch.Size([16, 256])\n",
      "First item's token_type_ids (should see 0s then 1s):\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Decoded first item:\n",
      "[CLS] i cannot believe this movie managed to get such a relatively high rating of it is barely watchable and unbelievably boring certainly one of the worst films i have seen in a long long timein a nobudget way it reminded me of star wars episodes i and ii for the sheer impression that you are watching a total creative train wreckthis film should be avoided at all costs it is one of those festival films that only please the pseudointellectuals because they are so badly made those people think it makes it different therefore goodbad filmmaking is not different it is just bad filmmaking [SEP] this review expresses negative sentiment. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT4TC Model instantiated (ready to use sentence-pair input).\n",
      "Optimizer with discriminative learning rates configured.\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 256 # Try a larger value if your reviews are long\n",
    "truncation=True\n",
    "\n",
    "class IMDbBERT4TCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert numerical label back to its original string form for auxiliary sentence generation\n",
    "        # This assumes your sentiment_mapping is accessible, or you pass the string label directly\n",
    "        # For simplicity here, we'll assume the original labels were 'positive' and 'negative'\n",
    "        original_label_str = 'positive' if label == 1 else 'negative' # If label is 0 or 1\n",
    "        auxiliary_sentence = get_auxiliary_sentence(label) # Pass numerical label to get aux sentence based on paper's idea\n",
    "\n",
    "        # KEY CHANGE: Pass text and auxiliary_sentence as a pair to the tokenizer\n",
    "        # This will create [CLS] review_text [SEP] auxiliary_sentence [SEP]\n",
    "        encoded_input = self.tokenizer(\n",
    "            text,\n",
    "            auxiliary_sentence, # The second sequence\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoded_input['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded_input['attention_mask'].squeeze(),\n",
    "            'token_type_ids': encoded_input['token_type_ids'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "train_texts = X_train['review'].tolist()\n",
    "test_texts = X_test['review'].tolist()\n",
    "\n",
    "sentiment_mapping = {'positive': 1, 'negative': 0}\n",
    "train_labels = [sentiment_mapping[label] for label in y_train.tolist()]\n",
    "test_labels = [sentiment_mapping[label] for label in y_test.tolist()]\n",
    "\n",
    "\n",
    "train_dataset = IMDbBERT4TCDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "test_dataset = IMDbBERT4TCDataset(test_texts, test_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"\\nDatasets and DataLoaders created successfully with auxiliary sentence input. ✅\")\n",
    "print(f\"Number of samples in training dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in test dataset: {len(test_dataset)}\")\n",
    "\n",
    "# Verify a batch's token_type_ids to see the effect of sentence pairing\n",
    "print(\"\\nVerifying token_type_ids for a sample batch:\")\n",
    "for batch in train_dataloader:\n",
    "    print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Attention Mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"Token Type IDs shape (should show 0s and 1s): {batch['token_type_ids'].shape}\")\n",
    "    # Example to show the token_type_ids for the first item in the batch\n",
    "    print(\"First item's token_type_ids (should see 0s then 1s):\")\n",
    "    print(batch['token_type_ids'][0])\n",
    "    # Decode to see the actual text and auxiliary sentence\n",
    "    print(\"Decoded first item:\")\n",
    "    decoded_text = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=False)\n",
    "    print(decoded_text)\n",
    "    break\n",
    "\n",
    "# --- Model Definition (as you had it) ---\n",
    "class BERT4TCModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, # This will now be meaningful (0s for review, 1s for aux sentence)\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "# Ensure 'device' is defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERT4TCModel(MODEL_NAME, num_labels=2).to(device)\n",
    "print(\"\\nBERT4TC Model instantiated (ready to use sentence-pair input).\")\n",
    "\n",
    "# --- Optimizer and Scheduler setup (as you had it, no changes needed for this part) ---\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "embeddings_params = []\n",
    "encoder_layer_params_lower = []\n",
    "encoder_layer_params_upper = []\n",
    "pooler_classifier_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "layer_pattern = re.compile(r'bert\\.encoder\\.layer\\.(\\d+)\\.')\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if any(nd in n for nd in no_decay):\n",
    "        no_decay_params.append(p)\n",
    "    else:\n",
    "        if 'bert.embeddings' in n:\n",
    "            embeddings_params.append(p)\n",
    "        elif 'bert.encoder.layer.' in n:\n",
    "            match = layer_pattern.search(n)\n",
    "            if match:\n",
    "                layer_idx = int(match.group(1))\n",
    "                if 0 <= layer_idx < 6:\n",
    "                    encoder_layer_params_lower.append(p)\n",
    "                elif 6 <= layer_idx < 12:\n",
    "                    encoder_layer_params_upper.append(p)\n",
    "        elif 'bert.pooler' in n or 'classifier' in n:\n",
    "            pooler_classifier_params.append(p)\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': embeddings_params,\n",
    "        'lr': LEARNING_RATE * 0.1,\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': encoder_layer_params_lower,\n",
    "        'lr': LEARNING_RATE * 0.3,\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': encoder_layer_params_upper,\n",
    "        'lr': LEARNING_RATE * 0.6,\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': pooler_classifier_params,\n",
    "        'lr': LEARNING_RATE,\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': no_decay_params,\n",
    "        'lr': 0.0,\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters)\n",
    "print(\"Optimizer with discriminative learning rates configured.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8ed2b",
   "metadata": {},
   "source": [
    "# Bert4TC Finetuning according to Paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0572ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate scheduler configured.\n",
      "All BERT encoder layers frozen.\n",
      "\n",
      "--- Epoch 1/1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061a5f4eee2c40749abf4a189c079cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.8408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ccb4916f004fffa471e8dd5509fa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Epoch 1:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.8652\n",
      "Validation Accuracy: 0.5150\n",
      "New best model saved with accuracy: 0.5150 at mlflow_models/best_sentiment_model_final.pth\n",
      "\n",
      "Training complete.\n",
      "Final best validation accuracy: 0.5150\n",
      "Best model saved at: mlflow_models/best_sentiment_model_final.pth\n",
      "🏃 View run nebulous-pig-609 at: http://mlflow:5000/#/experiments/0/runs/55b3577c2c194a1098e47085b7f8d984\n",
      "🧪 View experiment at: http://mlflow:5000/#/experiments/0\n",
      "\n",
      "MLflow run completed.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os # Make sure os is imported\n",
    "\n",
    "# Define your total epochs\n",
    "NUM_FREEZE_EPOCHS = 1\n",
    "NUM_GRADUAL_UNFREEZE_EPOCHS = 0\n",
    "TOTAL_EPOCHS = NUM_FREEZE_EPOCHS + NUM_GRADUAL_UNFREEZE_EPOCHS + 0\n",
    "\n",
    "# Calculate total steps for the scheduler using the actual TOTAL_EPOCHS\n",
    "total_steps = len(train_dataloader) * TOTAL_EPOCHS\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "# Initialize Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "print(\"Learning rate scheduler configured.\")\n",
    "\n",
    "\n",
    "# --- Gradual Unfreezing Helper Functions (as defined previously) ---\n",
    "def freeze_all_bert_layers(model):\n",
    "    \"\"\"Freezes all parameters within the BERT encoder layers.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bert.\" in name: # Targets parameters belonging to the BERT base model\n",
    "            param.requires_grad = False\n",
    "    print(\"All BERT encoder layers frozen.\")\n",
    "\n",
    "def unfreeze_bert_layer_block(model, layer_num):\n",
    "    \"\"\"Unfreezes a specific BERT encoder layer block.\"\"\"\n",
    "    layer_prefix = f\"bert.encoder.layer.{layer_num}.\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if layer_prefix in name:\n",
    "            param.requires_grad = True\n",
    "    print(f\"Unfrozen BERT layer {layer_num}.\")\n",
    "\n",
    "def unfreeze_bert_embeddings(model):\n",
    "    \"\"\"Unfreezes BERT's embedding layer.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bert.embeddings.\" in name:\n",
    "            param.requires_grad = True\n",
    "    print(\"Unfrozen BERT embeddings.\")\n",
    "\n",
    "# --- Initial Freezing (Before the loop starts) ---\n",
    "freeze_all_bert_layers(model)\n",
    "\n",
    "# --- MLflow Setup ---\n",
    "# Set the MLflow tracking URI (if not set by MLFLOW_TRACKING_URI environment variable)\n",
    "# By default, it's 'mlruns' in your current directory or can be a remote server.\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\") # CORRECTED LINE: Removed duplicate \"http://\"\n",
    "\n",
    "# Ensure the directory for saving models exists\n",
    "model_save_dir = \"mlflow_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(model_save_dir, 'best_sentiment_model_final.pth')\n",
    "\n",
    "\n",
    "# --- Start MLflow Run ---\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"num_freeze_epochs\", NUM_FREEZE_EPOCHS)\n",
    "    mlflow.log_param(\"num_gradual_unfreeze_epochs\", NUM_GRADUAL_UNFREEZE_EPOCHS)\n",
    "    mlflow.log_param(\"total_epochs\", TOTAL_EPOCHS)\n",
    "    mlflow.log_param(\"warmup_steps_ratio\", 0.1) # Log the ratio used for warmup steps\n",
    "    mlflow.log_param(\"initial_learning_rate\", optimizer.param_groups[0]['lr']) # Log initial LR\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # --- Training Loop without Checkpointing ---\n",
    "    for epoch in range(TOTAL_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{TOTAL_EPOCHS} ---\")\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Gradual Unfreezing Logic\n",
    "        if epoch == 0:\n",
    "            pass\n",
    "        elif epoch == NUM_FREEZE_EPOCHS:\n",
    "            unfreeze_bert_layer_block(model, 11)\n",
    "        elif epoch == NUM_FREEZE_EPOCHS + 1:\n",
    "            unfreeze_bert_layer_block(model, 10)\n",
    "        elif epoch == NUM_FREEZE_EPOCHS + 2:\n",
    "            unfreeze_bert_layer_block(model, 9)\n",
    "            unfreeze_bert_embeddings(model)\n",
    "            print(\"All specified BERT layers and embeddings unfrozen for subsequent epochs.\")\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step() # This will now be defined\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        mlflow.log_metric(\"avg_train_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        # --- Evaluation Loop ---\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        for batch in tqdm(test_dataloader, desc=f\"Evaluating Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_eval_accuracy += (preds == labels).sum().item()\n",
    "\n",
    "        avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
    "        avg_eval_accuracy = total_eval_accuracy / len(test_dataset)\n",
    "        print(f\"Average Validation Loss: {avg_eval_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {avg_eval_accuracy:.4f}\")\n",
    "\n",
    "        mlflow.log_metric(\"avg_eval_loss\", avg_eval_loss, step=epoch)\n",
    "        mlflow.log_metric(\"avg_eval_accuracy\", avg_eval_accuracy, step=epoch)\n",
    "\n",
    "        # --- Save the best model ---\n",
    "        if avg_eval_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_eval_accuracy\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"New best model saved with accuracy: {best_accuracy:.4f} at {model_save_path}\")\n",
    "            # Log the best model as an artifact\n",
    "            mlflow.log_artifact(model_save_path, \"best_model\")\n",
    "            mlflow.set_tag(\"best_accuracy\", f\"{best_accuracy:.4f}\") # Set a tag for easy filtering\n",
    "        else:\n",
    "            print(f\"Validation accuracy did not improve. Best so far: {best_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    print(f\"Final best validation accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Best model saved at: {model_save_path}\")\n",
    "\n",
    "    # Log the final best accuracy as a metric at the end of the run\n",
    "    mlflow.log_metric(\"final_best_accuracy\", best_accuracy)\n",
    "\n",
    "print(\"\\nMLflow run completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb459c",
   "metadata": {},
   "source": [
    "# Inference with Fine tuned bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b57e6c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from bert-base-uncased and model from mlflow_models/best_sentiment_model_final.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n",
      "Model loaded and set to evaluation mode. Ready for inference.\n",
      "\n",
      "--- Performing Inference on Example Reviews ---\n",
      "Review: 'This movie was absolutely brilliant! A masterpiece from start to finish. I loved every single moment.'\n",
      "Predicted Sentiment: negative (Confidence: 0.7372)\n",
      "\n",
      "Review: 'Worst film I've seen all year. The plot made no sense, and the acting was terrible. Don't waste your money.'\n",
      "Predicted Sentiment: negative (Confidence: 0.8038)\n",
      "\n",
      "Review: 'It was okay, nothing special. I wouldn't watch it again but it wasn't awful.'\n",
      "Predicted Sentiment: negative (Confidence: 0.7756)\n",
      "\n",
      "Review: 'The movie had some good parts, but overall it was a bit disappointing. The ending felt rushed.'\n",
      "Predicted Sentiment: negative (Confidence: 0.8118)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re # For preprocessing functions\n",
    "\n",
    "# --- Configuration (Must match training configuration) ---\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LENGTH = 256 # Make sure this matches your training MAX_LENGTH\n",
    "NUM_LABELS = 2 # 0 for negative, 1 for positive\n",
    "model_save_dir = \"mlflow_models\" # Directory where your best model is saved\n",
    "model_path = os.path.join(model_save_dir, 'best_sentiment_model_final.pth')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Define your BERT4TCModel class (Exact copy from your training script) ---\n",
    "class BERT4TCModel(torch.nn.Module):\n",
    "    def __init__(self, model_name_or_path, num_labels=2):\n",
    "        super().__init__()\n",
    "        # If loading from a saved state_dict, you first need to load the base pre-trained model\n",
    "        # and then load your fine-tuned weights onto it.\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "# --- Preprocessing functions (Exact copies from your data preparation) ---\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "def remove_contractions(text):\n",
    "    # This is a very basic example; a full implementation would be much larger\n",
    "    # You should use the exact same dictionary you used during training\n",
    "    contractions = {\n",
    "        \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "        \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "        \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\",\n",
    "        \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expansion, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def get_auxiliary_sentence_inference(sentiment_label_str):\n",
    "    \"\"\"\n",
    "    Generates the auxiliary sentence based on the sentiment for BERT4TC inference.\n",
    "    \"\"\"\n",
    "    if sentiment_label_str == \"positive\":\n",
    "        return \"This review expresses positive sentiment.\"\n",
    "    elif sentiment_label_str == \"negative\":\n",
    "        return \"This review expresses negative sentiment.\"\n",
    "    else:\n",
    "        # For BERT4TC, you typically only have positive/negative auxiliary sentences.\n",
    "        # If your model supports neutral, you'd add it here.\n",
    "        # For IMDb, it's usually binary.\n",
    "        raise ValueError(\"Invalid sentiment label string for auxiliary sentence generation.\")\n",
    "\n",
    "# --- Load the saved tokenizer and model ---\n",
    "print(f\"Loading tokenizer from {MODEL_NAME} and model from {model_path}...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "loaded_model = BERT4TCModel(MODEL_NAME, num_labels=NUM_LABELS).to(device)\n",
    "\n",
    "# Load the state_dict into your model\n",
    "if os.path.exists(model_path):\n",
    "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error: Model weights not found at {model_path}. Please ensure training was completed and model saved.\")\n",
    "    exit() # Exit if model weights are not found\n",
    "\n",
    "loaded_model.eval() # Set the model to evaluation mode\n",
    "print(\"Model loaded and set to evaluation mode. Ready for inference.\")\n",
    "\n",
    "\n",
    "# --- Inference Function ---\n",
    "def predict_sentiment(review_text, model, tokenizer, max_length, device):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a movie review using the fine-tuned BERT4TC model.\n",
    "    It applies the BERT4TC inference strategy by comparing probabilities\n",
    "    from positive and negative auxiliary sentences.\n",
    "    \"\"\"\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "    # Preprocess the input review text just like during training\n",
    "    processed_text = review_text.lower()\n",
    "    processed_text = remove_html_tags(processed_text)\n",
    "    processed_text = remove_url(processed_text)\n",
    "    processed_text = remove_contractions(processed_text)\n",
    "\n",
    "    # --- BERT4TC Inference Strategy ---\n",
    "    # We will get predictions for the review paired with a \"positive\" auxiliary sentence\n",
    "    # and separately for the review paired with a \"negative\" auxiliary sentence.\n",
    "    # The final prediction is based on which pairing yields a higher probability\n",
    "    # for the corresponding sentiment label.\n",
    "\n",
    "    # 1. Evaluate with \"positive\" auxiliary sentence\n",
    "    aux_pos = get_auxiliary_sentence_inference(\"positive\")\n",
    "    encoded_pos = tokenizer(\n",
    "        processed_text,\n",
    "        aux_pos, # Second sequence is the auxiliary sentence\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_pos = model(\n",
    "            input_ids=encoded_pos['input_ids'],\n",
    "            attention_mask=encoded_pos['attention_mask'],\n",
    "            token_type_ids=encoded_pos['token_type_ids']\n",
    "        )\n",
    "        logits_pos = outputs_pos.logits\n",
    "        # Get probability for the 'positive' class (label 1) when paired with 'positive' aux\n",
    "        # Assuming label 1 maps to positive\n",
    "        prob_review_is_positive_given_positive_aux = F.softmax(logits_pos, dim=1)[0][1].item()\n",
    "\n",
    "    # 2. Evaluate with \"negative\" auxiliary sentence\n",
    "    aux_neg = get_auxiliary_sentence_inference(\"negative\")\n",
    "    encoded_neg = tokenizer(\n",
    "        processed_text,\n",
    "        aux_neg, # Second sequence is the auxiliary sentence\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_neg = model(\n",
    "            input_ids=encoded_neg['input_ids'],\n",
    "            attention_mask=encoded_neg['attention_mask'],\n",
    "            token_type_ids=encoded_neg['token_type_ids']\n",
    "        )\n",
    "        logits_neg = outputs_neg.logits\n",
    "        # Get probability for the 'negative' class (label 0) when paired with 'negative' aux\n",
    "        # Assuming label 0 maps to negative\n",
    "        prob_review_is_negative_given_negative_aux = F.softmax(logits_neg, dim=1)[0][0].item()\n",
    "\n",
    "    # 3. Compare the probabilities to make the final decision\n",
    "    if prob_review_is_positive_given_positive_aux > prob_review_is_negative_given_negative_aux:\n",
    "        return \"positive\", prob_review_is_positive_given_positive_aux\n",
    "    else:\n",
    "        return \"negative\", prob_review_is_negative_given_negative_aux\n",
    "\n",
    "# --- Test your model with example reviews ---\n",
    "print(\"\\n--- Performing Inference on Example Reviews ---\")\n",
    "\n",
    "review1 = \"This movie was absolutely brilliant! A masterpiece from start to finish. I loved every single moment.\"\n",
    "sentiment1, confidence1 = predict_sentiment(review1, loaded_model, tokenizer, MAX_LENGTH, device)\n",
    "print(f\"Review: '{review1}'\")\n",
    "print(f\"Predicted Sentiment: {sentiment1} (Confidence: {confidence1:.4f})\\n\")\n",
    "\n",
    "review2 = \"Worst film I've seen all year. The plot made no sense, and the acting was terrible. Don't waste your money.\"\n",
    "sentiment2, confidence2 = predict_sentiment(review2, loaded_model, tokenizer, MAX_LENGTH, device)\n",
    "print(f\"Review: '{review2}'\")\n",
    "print(f\"Predicted Sentiment: {sentiment2} (Confidence: {confidence2:.4f})\\n\")\n",
    "\n",
    "review3 = \"It was okay, nothing special. I wouldn't watch it again but it wasn't awful.\"\n",
    "sentiment3, confidence3 = predict_sentiment(review3, loaded_model, tokenizer, MAX_LENGTH, device)\n",
    "print(f\"Review: '{review3}'\")\n",
    "print(f\"Predicted Sentiment: {sentiment3} (Confidence: {confidence3:.4f})\\n\")\n",
    "\n",
    "review4 = \"The movie had some good parts, but overall it was a bit disappointing. The ending felt rushed.\"\n",
    "sentiment4, confidence4 = predict_sentiment(review4, loaded_model, tokenizer, MAX_LENGTH, device)\n",
    "print(f\"Review: '{review4}'\")\n",
    "print(f\"Predicted Sentiment: {sentiment4} (Confidence: {confidence4:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66cdead",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "My groupmate Iqra Rathore (SP25-Rai-018) and I, Sumair Javed (SP25-Rai-019), worked on a project to teach a smart computer program called BERT how to figure out if movie reviews from the IMDb website are good (positive) or bad (negative). We did this by fine-tuning BERT. The fine-tuning techniques employed were standard fine-tuning and the BERT4TC method with gradual unfreezing.\n",
    "\n",
    "This project took a lot of time because we faced a big problem early on: our first IMDb dataset was wrong. This made our program overfit, meaning it became too good at remembering our training examples but couldn't guess correctly on new reviews. It was tough to find the right dataset, and fixing this took quite a while before we could even properly train the program.\n",
    "\n",
    "We tried two main ways to fine-tune BERT:\n",
    "\n",
    "## Our Regular Way:\n",
    "First, we cleaned up the movie reviews by removing things like internet links and fixing shortened words. Then, we just fine-tuned BERT directly on these clean reviews.\n",
    "\n",
    "## The Paper's Way (BERT4TC):\n",
    "We also tried a more advanced method from a research paper. This involved adding a special \"helper sentence\" to each movie review. For example, if a review was positive, we'd add, \"This review shows good feelings.\" Then, we fine-tuned BERT using both the review and this helper sentence together. This helped BERT understand the sentiment better.\n",
    "\n",
    "However, training the Paper's Way was very slow. Even one round of training (called an epoch) took about two hours. Because we could only run it for one epoch, our model either didn't learn enough or it became too focused on guessing \"negative\" every time. This showed us that training for too short a time meant the model wasn't properly fine-tuned.\n",
    "\n",
    "For both methods, we made sure to prepare the data carefully so BERT could learn as well as possible, despite the early challenges and the long training times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
